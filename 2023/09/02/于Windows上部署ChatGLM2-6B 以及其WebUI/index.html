<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  <!-- google analytics -->
  <meta name="google-site-verification" content="DcvKYP3aUT2r5C_9liWXDU7Hfaj8-0llUB0-Wr0J9l8" />
  <!-- google analytics end -->
  <!-- baidu analytics -->
  <meta name="baidu-site-verification" content="codeva-EnHVSvuTxv" />
  <!-- baidu analytics end -->
  <!-- bing analytics -->
  <meta name="msvalidate.01" content="59B362C3958F9B77C547452838F82199" />
  <!-- bing analytics end -->
  
  <title>于Windows上部署ChatGLM2-6B 以及其WebUI | 容小狸的博客</title>
  <meta name="author" content="Rong Xiaoli">
  
  <meta name="description" content="寻思要给自己搞一个能上网的GLM2-6B，就在GLM官网找有没有友链，正好找到了这么一条：ChatGLM-6B-Engineering，于是就打算在本地部署一下（我是比较讨厌语言模型不在本地的那种）。坑有点多，于是用几乎是0基础的方式写了这么一篇教程。本篇文章使用venv来创建虚拟环境，conda创建环境的可以找别人了。

0x0 拉代码到这个工程的仓库拉下代码，解压到一个文件夹。创建一个命令行窗口，然后cd到你的目录。12rem Go to your working dir. cd /D D:\path\to\your\workspace0x1 创建环境然后创建python虚拟环境（有些人极度讨厌虚拟环境可以跳过，此处使用venv而不是conda是因为我比较讨厌conda）：12rem Run python to create venv. python -m venv .\venv确保缓存的盘足够，如果不能的话，就会报pip没有足够的空间安装，你需要这么做：12rem Set the cache dir. set TMPDIR=D:\your\cache\dir然后再在同一窗口安装执行：pip安装依赖：1pip install -r requirements.txt0x2 使用模型搭建GLM层API接下来GLM模型就已经基本可以使用了。ChatGLM有量化，可以在稍微性能不足一点的电脑上运行：以下内容摘自ChatGLM-6B的README.md:
硬件需求


量化等级
最低 GPU 显存（推理）
最低 GPU 显存（高效参数微调）



FP16（无量化）
13 GB
14 GB


INT8
8 GB
9 GB


INT4
6 GB
7 GB"> 
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="于Windows上部署ChatGLM2-6B 以及其WebUI"/>
  <meta property="og:site_name" content="容小狸的博客"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="容小狸的博客" type="application/atom+xml">
  
  
    <link href="/fa fa-rss" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/prism.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-70812759-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-70812759-1');
</script>






<meta name="generator" content="Hexo 6.3.0"></head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">容小狸的博客</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="全部文档">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="全部分类">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="全部标签">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="我？我有什么值得注意的？">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="其实我自己都不用RSS">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 于Windows上部署ChatGLM2-6B 以及其WebUI</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>寻思要给自己搞一个能上网的GLM2-6B，就在<a target="_blank" rel="noopener" href="https://chatglm.cn/blog">GLM官网</a>找有没有友链，正好找到了这么一条：<a target="_blank" rel="noopener" href="https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering">ChatGLM-6B-Engineering</a>，于是就打算在本地部署一下（我是比较讨厌语言模型不在本地的那种）。坑有点多，于是用几乎是0基础的方式写了这么一篇教程。<br>本篇文章使用venv来创建虚拟环境，conda创建环境的可以找别人了。</p>
<hr>
<h2 id="0x0-拉代码"><a href="#0x0-拉代码" class="headerlink" title="0x0 拉代码"></a>0x0 拉代码</h2><h2 id="到这个工程的仓库拉下代码，解压到一个文件夹。创建一个命令行窗口，然后cd到你的目录。"><a href="#到这个工程的仓库拉下代码，解压到一个文件夹。创建一个命令行窗口，然后cd到你的目录。" class="headerlink" title="到这个工程的仓库拉下代码，解压到一个文件夹。创建一个命令行窗口，然后cd到你的目录。"></a>到这个工程的仓库拉下代码，解压到一个文件夹。<br>创建一个命令行窗口，然后cd到你的目录。<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">rem Go to your working dir. </span></span><br><span class="line"><span class="built_in">cd</span> /D D:\<span class="built_in">path</span>\to\your\workspace</span><br></pre></td></tr></table></figure></h2><h2 id="0x1-创建环境"><a href="#0x1-创建环境" class="headerlink" title="0x1 创建环境"></a>0x1 创建环境</h2><h2 id="然后创建python虚拟环境（有些人极度讨厌虚拟环境可以跳过，此处使用venv而不是conda是因为我比较讨厌conda）：确保缓存的盘足够，如果不能的话，就会报pip没有足够的空间安装，你需要这么做：然后再在同一窗口安装执行：pip安装依赖："><a href="#然后创建python虚拟环境（有些人极度讨厌虚拟环境可以跳过，此处使用venv而不是conda是因为我比较讨厌conda）：确保缓存的盘足够，如果不能的话，就会报pip没有足够的空间安装，你需要这么做：然后再在同一窗口安装执行：pip安装依赖：" class="headerlink" title="然后创建python虚拟环境（有些人极度讨厌虚拟环境可以跳过，此处使用venv而不是conda是因为我比较讨厌conda）：确保缓存的盘足够，如果不能的话，就会报pip没有足够的空间安装，你需要这么做：然后再在同一窗口安装执行：pip安装依赖："></a>然后创建python虚拟环境（有些人极度讨厌虚拟环境可以跳过，此处使用venv而不是conda是因为我比较讨厌conda）：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">rem Run python to create venv. </span></span><br><span class="line">python -m venv .\venv</span><br></pre></td></tr></table></figure><br>确保缓存的盘足够，如果不能的话，就会报pip没有足够的空间安装，你需要这么做：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">rem Set the cache dir. </span></span><br><span class="line"><span class="built_in">set</span> TMPDIR=D:\your\cache\<span class="built_in">dir</span></span><br></pre></td></tr></table></figure><br>然后再在同一窗口安装执行：<br>pip安装依赖：<br><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></h2><h2 id="0x2-使用模型搭建GLM层API"><a href="#0x2-使用模型搭建GLM层API" class="headerlink" title="0x2 使用模型搭建GLM层API"></a>0x2 使用模型搭建GLM层API</h2><p>接下来GLM模型就已经基本可以使用了。<br>ChatGLM有量化，可以在稍微性能不足一点的电脑上运行：<br>以下内容摘自<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B#%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F">ChatGLM-6B的README.md</a>:</p>
<h3 id="硬件需求"><a href="#硬件需求" class="headerlink" title="硬件需求"></a>硬件需求</h3><table>
<thead>
<tr>
<th><strong>量化等级</strong></th>
<th><strong>最低 GPU 显存</strong>（推理）</th>
<th><strong>最低 GPU 显存</strong>（高效参数微调）</th>
</tr>
</thead>
<tbody><tr>
<td>FP16（无量化）</td>
<td>13 GB</td>
<td>14 GB</td>
</tr>
<tr>
<td>INT8</td>
<td>8 GB</td>
<td>9 GB</td>
</tr>
<tr>
<td>INT4</td>
<td>6 GB</td>
<td>7 GB</td>
</tr>
</tbody></table>
<p>实在不行还可以在CPU上运行，这个稍后会提到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).quantize(<span class="number">4</span>).half().cuda()</span><br></pre></td></tr></table></figure>
<p>上面的代码中，表示量化的在这里：<code>.quantize(4)</code>，这表示该模型使用INT4量化，同理<code>.quantize(8)</code>就是使用INT8量化。<br>但是使用他的API还需要改一点东西：<br>他的模型使用本地模型，我们把它改成线上模型。<br>改一行代码：<br><code>.\api.py</code><br>更改前：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(&quot;THUDM/chatglm2-6b&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(&quot;THUDM/chatglm2-6b&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">r&quot;E:\model\chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">r&quot;E:\model\chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).quantize(<span class="number">4</span>).half().cuda()</span><br></pre></td></tr></table></figure>
<p>更改后：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).quantize(<span class="number">4</span>).half().cuda()</span><br><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(r&quot;E:\model\chatglm2-6b&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(r&quot;E:\model\chatglm2-6b&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br></pre></td></tr></table></figure>
<p>现在就是线上模型了。但是按照他写的，我们现在实际上用的是CPU处理。还是使用上面的代码会报错，这里我们分两种情况：</p>
<h3 id="1、使用CPU进行推理："><a href="#1、使用CPU进行推理：" class="headerlink" title="1、使用CPU进行推理："></a>1、使用CPU进行推理：</h3><p>更改刚刚那段代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="literal">True</span>).quantize(<span class="number">4</span>).half().<span class="built_in">float</span>()</span><br><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(r&quot;E:\huggingface\models--THUDM--chatglm-6b\snapshots\a10da4c68b5d616030d3531fc37a13bb44ea814d&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br><span class="line"><span class="comment">#tokenizer = AutoTokenizer.from_pretrained(r&quot;E:\model\chatglm2-6b&quot;, trust_remote_code=True)</span></span><br><span class="line"><span class="comment">#model = AutoModel.from_pretrained(r&quot;E:\model\chatglm2-6b&quot;, trust_remote_code=True).quantize(4).half().cuda()</span></span><br></pre></td></tr></table></figure>
<p>现在我们就可以在CPU上使用ChatGLM了。</p>
<h3 id="2、使用CUDA进行推理："><a href="#2、使用CUDA进行推理：" class="headerlink" title="2、使用CUDA进行推理："></a>2、使用CUDA进行推理：</h3><p>代码无需更改，但是我们需要更改torch的版本。现在是CPU版本，我们需要卸载并安装GPU版本。</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall torch</span><br></pre></td></tr></table></figure>
<h4 id="2-1-你没有CUDA："><a href="#2-1-你没有CUDA：" class="headerlink" title="2.1 你没有CUDA："></a>2.1 你没有CUDA：</h4><p>没有安装CUDA的按照以下步骤下载并安装：<br>首先检查NVidia的显卡支持的CUDA版本：</p>
<ul>
<li>右键NVidia设置；</li>
<li>点击NVidia控制面板；</li>
<li>点击“帮助”-“系统信息”；</li>
<li>点击“组件”；</li>
<li>查看“3D 设置”-“NVCUDA64.DLL”-“产品名称”<br>想必电脑应该基本都是x64的CPU，如果还是x86的我也不知道怎么办，换电脑吧……<br>下载的CUDA版本不得低于产品名称里显示的版本。于是我下载了<a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda_12.2.2_537.13_windows.exe">11.7的本地安装版</a><br>最新CUDA在<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">这里</a>下载。<br>安装的时候，可以精简，如果没地方安装可以选择自定义，然后只选择CUDA，甚至还可以把CUDA的Document、Visual Studio支持也取消选择。<br>选择安装位置，点击安装。<br>接下来按照2.2走：</li>
</ul>
<h4 id="2-2-你有CUDA："><a href="#2-2-你有CUDA：" class="headerlink" title="2.2 你有CUDA："></a>2.2 你有CUDA：</h4><p>torch官网，找到Install PyTorch，按照实际情况选择：<br>我的情况：<br>系统：Windows<br>包安装器：Pip<br>语言：Python<br>计算平台：CUDA 11.7<br>那么就有：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117</span><br></pre></td></tr></table></figure>
<p>记得以上命令在虚拟环境中执行。</p>
<p>然后我们可以这样验证：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python</span><br></pre></td></tr></table></figure>
<p>在python交互式终端中输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure>
<p>如果输出为True，那么我们就安装好了。我们正常使用就OK了。</p>
<h3 id="3、batch脚本一键启动"><a href="#3、batch脚本一键启动" class="headerlink" title="3、batch脚本一键启动"></a>3、batch脚本一键启动</h3><p>从<code>.\venv\Scripts\activate.bat</code>中，复制所有文本，在工作目录下新建<code>api.bat</code>，粘贴。<br>脚本：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...(这一段是你的activate.bat中的内容)</span><br><span class="line"><span class="comment">rem 更改标题</span></span><br><span class="line"><span class="built_in">title</span> ChatGLM-<span class="number">6</span>B WebUI API</span><br><span class="line"><span class="comment">rem 启动GLM层API</span></span><br><span class="line">python .\api.py</span><br><span class="line"><span class="built_in">pause</span></span><br></pre></td></tr></table></figure>
<p>接下来是交互层API：</p>
<h2 id="0x3-搭建交互层API"><a href="#0x3-搭建交互层API" class="headerlink" title="0x3 搭建交互层API"></a>0x3 搭建交互层API</h2><p>这部分其实很简单，只需要更改一下他的插件里面的浏览器内核的配置就可以了：</p>
<ul>
<li>打开<code>.\plugins\web.py</code></li>
<li>更换所有的不在字符串里的<code>Chrome</code>为<code>Edge</code>，为的是使所有Chrome配置更改为Edge配置</li>
<li>保存<br>就搞定了。</li>
</ul>
<p>一键启动基本一样：<br>从<code>.\venv\Scripts\activate.bat</code>中，复制所有文本，在工作目录下新建<code>front_end.bat</code>，粘贴。<br>脚本：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...(这一段是你的activate.bat中的内容)</span><br><span class="line"><span class="comment">rem 更改标题</span></span><br><span class="line"><span class="built_in">title</span> ChatGLM-<span class="number">6</span>B WebUI front end</span><br><span class="line"><span class="comment">rem 启动交互层API</span></span><br><span class="line">python .\front_end.py</span><br><span class="line"><span class="built_in">pause</span></span><br></pre></td></tr></table></figure>
<h2 id="0x4-搭建前端"><a href="#0x4-搭建前端" class="headerlink" title="0x4 搭建前端"></a>0x4 搭建前端</h2><p>有两种前端，一个是在项目主分支里写好的，已经被我们下载下来的前端，还有一种是另一根类Open AI式的前端。我会分开讲：</p>
<h3 id="1-使用已下载的前端"><a href="#1-使用已下载的前端" class="headerlink" title="1. 使用已下载的前端"></a>1. 使用已下载的前端</h3><p>一键启动就行：<br>从<code>.\venv\Scripts\activate.bat</code>中，复制所有文本，在工作目录下新建<code>web.bat</code>，粘贴。<br>脚本：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...(这一段是你的activate.bat中的内容)</span><br><span class="line"><span class="comment">rem 更改标题</span></span><br><span class="line"><span class="built_in">title</span> ChatGLM-<span class="number">6</span>B gradio demo</span><br><span class="line"><span class="comment">rem 启动GLM层API</span></span><br><span class="line">python .\gradio_demo.py</span><br><span class="line"><span class="built_in">pause</span></span><br></pre></td></tr></table></figure>
<h3 id="2-使用类Open-AI的前端"><a href="#2-使用类Open-AI的前端" class="headerlink" title="2. 使用类Open AI的前端"></a>2. 使用类Open AI的前端</h3><p>从<a target="_blank" rel="noopener" href="https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering/archive/refs/heads/WebUI.zip">这里</a>下载，并解压缩到一个文件夹内，我个人习惯丢到工作目录下的新文件夹：<code>.\WebUI</code><br>解压缩之后，<a target="_blank" rel="noopener" href="https://nodejs.org/download/release/v14.21.3/node-v14.21.3-x64.msi">下载</a>并安装node.js 14.21.3。<br>切换到WebUI工作目录并运行以下指令：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure>
<p>等安装完毕之后，打开<code>src\App.vue</code>，并修改：</p>
<ul>
<li>把所有的<code>process.env.VUE_APP_API</code>替换为<code>&quot;http://127.0.0.1:8003&quot;</code></li>
<li>保存<br>然后在创建一个<code>web.bat</code>，输入以下内容：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@echo off</span><br><span class="line">rem 替换D:\your\path\to\webui为你的WebUI工作路径</span><br><span class="line">cd /D D:\your\path\to\webui</span><br><span class="line">title ChatGLM-6B Web UI</span><br><span class="line">npm run dev</span><br><span class="line">pause</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="0x5-修补"><a href="#0x5-修补" class="headerlink" title="0x5 修补"></a>0x5 修补</h2><ul>
<li><p>该插件使用markmap绘制思维导图<br>所以如果启用了markmap，你需要执行这个：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install markmap markmap-cli -g</span><br></pre></td></tr></table></figure>
</li>
<li><p>类Open AI前端的左侧标题有对于该主题的概括，但是概括的时候输入并没有被赋值，所以在</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat</span>(<span class="params">prompt: <span class="built_in">str</span></span>):</span><br></pre></td></tr></table></figure>
<p>这一句后面添加一行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chat_prompt = prompt <span class="comment"># This is temp fix. </span></span><br></pre></td></tr></table></figure>
<p>关于这一点我已经提交了issue，就看作者怎么处理这个了。</p>
</li>
</ul>
<hr>
<p>2023-09-03编辑：该bug已经修补了：<br><a target="_blank" rel="noopener" href="https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering/pull/31">A temp fix for sidebar title. 对于侧边栏标题的临时性修复。 by rong-xiaoli · Pull Request #31</a></p>
<ul>
<li>使用网络插件的时候，可能是因为某些bug，无法返回查询内容（可能连请求都发不出去），已经提交Issue：<a target="_blank" rel="noopener" href="https://github.com/LemonQu-GIT/ChatGLM-6B-Engineering/issues/32">无法使用网络搜索 · Issue #32</a><br>我的解决方案是从Chrome切换至Edge，也就是把除了UA部分的Chrome全部改成Edge（或者说把所有Chrome的实现改为Edge）</li>
</ul>
<h2 id="0x6-最后需要注意的点"><a href="#0x6-最后需要注意的点" class="headerlink" title="0x6 最后需要注意的点"></a>0x6 最后需要注意的点</h2><ul>
<li>需要先启动GLM层API，启动完成后再启动交互层API；前端随时可以启动；</li>
<li>本篇所有的目录请根据实际情况核实一遍，不要直接<code>Ctrl+C</code>，<code>Ctrl+V</code>就不管了；</li>
</ul>

	  <div class="article-footer-copyright">

    本博客采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议(CC BY-NC-SA 4.0) 发布.</a>
</div>

	</div>

	
	
	<div>
  	<center>

	<div class="pagination">

    
    
    <a href="/2023/10/10/pwnagotchi/" type="button" class="btn btn-default"><i
                class="fa fa-arrow-circle-o-left"></i> Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2023/08/04/电赛/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>


    </center>
	</div>
	
	<!-- comment -->
	<!--
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>

-->
	
		<section id="comments" class="comments">
			<style>
			.comments{margin:30px;padding:10px;background:rgb(0, 0, 0)}
			@media screen and (max-width:800px){.comments{margin:auto;padding:10px;background:#000}}
			</style>
			<div id="vcomment" class="comment"></div>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<script src="https://cdnjs.loli.net/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
<script>
var valineConfig = {"enable":true,"appId":"AjgVa7glgk4i6IEbtfhvLoeS-gzGzoHsz","appKey":"dRTSo5QbogBgKa5MHpQf9JLZ","placeholder":"","visitor":true,"avatar":"monsterid","requiredFields":["nick","mail"],"pageView":false,"recordIP":true}
valineConfig.el='#vcomment';
new Valine(valineConfig);
    // new Valine({
    //     el: '#vcomment',
    //     appId: "",
    //     appKey: "",
    //     placeholder: "",
    //     avatar:"monsterid",
    //     visitor: "true",
    //     requiredFields: "nick,mail".split(','),
    // });
</script>

		</section>
	
	
	
	</div> <!-- col-md-9/col-md-12 -->


	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2023-09-02 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/人工智能/">人工智能<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/人工智能/">人工智能<span>2</span></a></li> <li><a href="/tags/LLM/">LLM<span>1</span></a></li> <li><a href="/tags/Python/">Python<span>2</span></a></li> <li><a href="/tags/ChatGLM2-6B/">ChatGLM2-6B<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

		

	</div>
	
		

</div><!-- row -->

<!--
 -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  
  &copy; 2023 Rong Xiaoli's Blog
  
      powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.Theme <a href="https://github.com/Ares-X/hexo-theme-freemind.bithack" target="_blank">freemind.bithack</a>  
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
